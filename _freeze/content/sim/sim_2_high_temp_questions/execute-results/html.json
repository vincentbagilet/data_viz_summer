{
  "hash": "25e6a67371d7b1e7e8663bec4c3b2a2a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Replication and Calibration\"\nsubtitle: \"Replication of a paper on impacts of high temperature on agricultural yields.\"\npublished-title: \"Date\"\ndate: \"2024-10-09\"\nself-contained: true\nengine: knitr\nhtml-math-method: mathml\n---\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip title=\"Objective\"}\nReplicate and simulate a paper to allow us to evaluate the performance and robustness of the analysis.\n:::\n\nWe first load the packages used in this document.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code to load the R packages used in this doc\"}\nlibrary(tidyverse)\nlibrary(knitr) \nlibrary(kableExtra)\nlibrary(broom) \nlibrary(here)\nlibrary(fixest) #for fixed effect regression\nlibrary(haven) #to read Stata .dta files\n\nset.seed(35) #fix the starting point of the random numbers generator\n\n# The code below is to use my own package for ggplot themes\n# You can remove it. Otherwise you will need to install the package via GitHub\n# using the following line of code\n# devtools::install_github(\"vincentbagilet/mediocrethemes\")\nlibrary(mediocrethemes)\nset_mediocre_all(pal = \"portal\") #set theme for all the subsequent plots\n\n# For exercises\nlibrary(webexercises)\n```\n:::\n\n\n# Setting\n\nIn this example, we explore the **impact of climate change (or high temperatures) on agricultural yields**. Usual approaches to explore this question rely on variation over time to compare hot and cold conditions for a given area. This approach has been implemented in fear of omitted variable bias in traditional cross-sectional approaches (comparing hot areas to cold areas), climate being potentially correlated with other time invariant unobserved variables. \nA large set of papers leverage this approach to address this question, *e.g.* [Deschênes and Greenstone (2007)](https://www.aeaweb.org/articles?id=10.1257/aer.97.1.354), [Schlenker and Roberts (2009)](https://www.pnas.org/doi/10.1073/pnas.0906865106) or [Burke and Emerick (2016)](https://www.aeaweb.org/articles?id=10.1257/pol.20130025).\n\nIn the present analysis, we will build simulations to emulate a typical study from this literature, measuring the impact of high temperatures on agricultural yields. A typical approach consists in estimating the link between high Growing Degree Days (GDD, roughly the number of days a crop is exposed to a certain range of temperatures) and yields, controlling for characteristics that are invariant in time and across locations. As in Burke and Emerick (2016) and some specifications from Schlenker and Roberts (2009), we will model log corn yield as a piecewise linear function of temperature, considering high and low GDDs. Most of the analyses in the literature being at the county-year level, we will simulate data with a similar structure.\n\nTo explore the data generating process, we will leverage data from [Burke and Emerick (2016)](https://www.aeaweb.org/articles?id=10.1257/pol.20130025), henceforth BE. This will allow us to make a few simplifying assumptions regarding the data generating process. By replicating BE, we will try to confirm that these assumptions do not significantly alter the results of the estimation in an actual study and that simulations under these assumptions remain close to an actual study from this literature. \n\nLet's thus first replicate the analysis in BE.\n\n# Replication\n\n## Replication setting and set up\n\n[Burke and Emerick (2016)](https://www.aeaweb.org/articles?id=10.1257/pol.20130025), henceforth BE, aims to evaluate the impact of high temperature days on corn production, estimating the following equation:\n\n$$\nlog(y_{ct}) = \\beta_0 + \\beta_1 H_{ct} + \\beta_2 L_{ct} + \\beta_3 H^{prec}_{ct} + \\beta_4 L^{prec}_{ct} + \\lambda_c + \\kappa_t + u_{ct}\n$$ {#eq-main-model}\n\nwhere $y_{ct}$ is the average crop yield in county $c$ in year $t$, $L_{ct}$ the number of Growing Degree Days (GDDs)^[BE presents it as \"the amount of time a crop is exposed to temperatures between a given lower and upper bound\". [Schlenker and Roberts (2009)](https://www.pnas.org/doi/10.1073/pnas.0906865106) gives an example: for \"bounds of 8°C and 32°C [...] a day of 9°C contributes 1 degree day, a day of 10°C contributes 2 degree days, up to a temperature of 32°C, which contributes 24 degree days. All temperatures above 32°C also contribute 24 degree days. Degree days are then summed over the entire season.”] below a given threshold (28 and 29°C for the panel and long-differences models respectively), $H$ the number of GDDs above this threshold. $H^{prec}_{ct}$ and $L^{prec}_{ct}$ represent equivalent measures for precipitations (threshold = 42 cm/year for the panel model, 50 for the long-differences one). $\\lambda_c$ and $\\kappa_t$ are county and time fixed effects respectively and $u_{ct}$ an error term. \n\nThe results we want to replicate are displayed in the paper, in Table 1 column 3:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](data/replication_BE/table1_paper.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\nHere we are going to focus on the results from table 1 in BE, for the panel model. Their code is written in `Stata`; we can translate it to `R`. For simplicity and since it is not the purpose of this exercise, I already did that and cleaned the data.\n\nYou however need to download their [replication package](http://doi.org/10.3886/E114567V1) on [the paper's page](https://www.aeaweb.org/articles?id=10.1257/pol.20130025). Then store the data set `us_panel.dta` in your working directory (or a sub directory).\n\n::: callout-warning\nYou will need to change the path in `read_dta()` and adapt it to your case.\n:::\n\n\n::: {.cell layout-align=\"center\" code_folding='true'}\n\n```{.r .cell-code}\nBE_data <- haven::read_dta(\n  here::here(\"content\", \"sim\", \"data\", \"replication_BE\", \"us_panel.dta\"))\n\nBE_wrangled <- BE_data |>\n  filter(year >= 1978 & year <= 2002) |> \n  filter(longitude > -100) |> \n  transmute(\n    year,\n    fips = str_pad(fips, 5, \"left\", \"0\"),\n    stfips = str_sub(fips, 1, 2),\n    cornyield,\n    logcornyield = log(cornyield),\n    lower = dday0C - dday29C,\n    higher = dday29C,\n    prec_lo = ifelse(prec <= 42, prec - 42, 0),\n    prec_hi = ifelse(prec > 42, prec - 42, 0),\n    corn_area\n  ) \n\nBE_clean <- BE_wrangled |> \n  group_by(fips) |> \n  mutate(\n    #/sum to compute the equivalent of analytic weights in stata\n    corn_area_78_02 = mean(corn_area, na.rm = TRUE)/sum(!is.na(corn_area))\n  ) |> \n  ungroup()\n```\n:::\n\n\n## Data structure\n\nBefore anything, let's look at this data. It is a panel with a time dimension, <input class='webex-solveme nospaces' size='4' data-answer='[\"year\"]'/> and an individual dimension, <input class='webex-solveme nospaces' size='4' data-answer='[\"fips\"]'/>. \n\nHow many time periods are there in this panel? <input class='webex-solveme nospaces' size='2' data-answer='[\"25\"]'/> \n\nAnd how many individuals (counties)?  <input class='webex-solveme nospaces' size='4' data-answer='[\"2459\"]'/>\n\nIs this panel balanced? <select class='webex-select'><option value='blank'></option><option value=''>Yes</option><option value=''>Not at all</option><option value='answer'>Almost</option></select>.\n\n\n\n\n\nThe standard errors are clustered at the state level, we first have a quick look at this additional layer in the panel. How many states are there in this panel? <input class='webex-solveme nospaces' size='2' data-answer='[\"37\"]'/>. Is it equal to the number of states in the US? Why is that?\n\nHow many observations are there on average by state? (county/years) <input class='webex-solveme nospaces' data-tol='1' size='16' data-answer='[\"1659.64864864865\"]'/>. Is the number of observations per state uniformely distributed across states? <select class='webex-select'><option value='blank'></option><option value=''>Yes</option><option value='answer'>Not at all</option><option value=''>Almost</option></select> \n\n## Actual replication\n\nBefore anything, let's try to replicate the main regression, as specified in the paper. The model is the one described in @eq-main-model. \n\nSince the regression model specified in the paper is a fixed effect one, we are going to estimate it using the `feols()` function from the `fixest` package. Look at the documentation for this function. To access it, you can use the <input class='webex-solveme nospaces' size='6' data-answer='[\"?\",\"help\",\"help()\"]'/> function. \n\n::: callout-warning\nTo be able to replicate the results in the analysis, we need to include weights and cluster the standard errors at the state level.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_fe <- feols(\n  ...,\n  weights = ~ corn_area_78_02, \n  cluster = ~ stfips\n)\n```\n:::\n\n\n\n\n## A simpler model?\n\nWe will generate data assuming that the econometric model we want to estimate accurately represents the DGP. However, this econometrics model is rather complex and it might be challenging to directly simulate a DGP based on it. Let's try to see if a simpler model would yield very different results.\n\nRemove the controls (*ie* only keeping one explanatory variable) and time fixed effects from your econometric model. Estimate this new econometric model. The explained variable is <input class='webex-solveme nospaces' size='12' data-answer='[\"logcornyield\"]'/>. **The** explanatory variable is <input class='webex-solveme nospaces' size='6' data-answer='[\"higher\"]'/> and the fixed effects are <input class='webex-solveme nospaces' size='4' data-answer='[\"fips\"]'/>. Let's also remove the weights for simplicity.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_fe_no_ctrl <- feols(\n  ...,\n  cluster = ~stfips\n)\n```\n:::\n\n\nAre the results approximately comparable to the full model? \n\nWe will therefore simulate this simplified model. How many variables do we need to generate? <input class='webex-solveme nospaces' size='1' data-answer='[\"4\"]'/>\n\n\n\n<!-- Based on these results, and compared to the literature, we therefore abstract from modeling: -->\n\n<!-- - Precipitations, to avoid having to simulate complex interactions with temperature.  -->\n<!-- - Time fixed effects. We only model county fixed effects in order to have only one varying parameter. However, we take time fixed effects into account: the distributions of the variables represent those of the observed variables *after removing time fixed effects*. This is equivalent to working on a version of the data in which these time fixed effects have already been partialled out.  -->\n<!-- - Growing degree days below the threshold, to simplify the data generating process, only considering those above ($HighGDD$), the variable of interest -->\n<!-- - Correlation between counties. While this makes the simulations less representative of real cases, it mostly improves and facilitates the estimation. We could complexify our simulations to include this correlation in a later stage. -->\n\n\n\n\n# Distribution of the variables\n\nTo be able to generate a fake data set, we first need to explore the distribution of  the variables.\n\n## Distribution the logcornyield\n\nWe first focus on the log of corn yields. Since the dependent variable in the analysis is the transformed version of corn yields, we will directly generate it, instead of generating cornyield and then taking its log. We start by plotting the distribution of log corn yields and fitting a normal to it, manually selecting the parameters of the normal distribution.\n\nLet's first plot the distribution of `logcornyield`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot\n```\n:::\n\n\nWhat could be a decent approximation of this distribution ? Using `geom_function()`, let's overlay the cdf of this theoretical distribution on top of the distribution of logcornyield. Manually adjust the parameters of the theoretical distribution to get a good fit.\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nBE_clean |> \n  ggplot(aes(x = logcornyield)) + \n  geom_density() + \n  geom_function(\n    fun = \\(x) dnorm(x, mean  = 4.55, sd = 0.33),\n      linewidth = 1\n    ) +\n  labs(\n    title = \"Distribution of log corn yields\",\n    subtitle = \"Tentative normal fit\",\n    x = \"Log Corn Yield\",\n    y = \"Denisty\"\n  )\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_questions_files/figure-html/distrib_logcornyield_fit-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n</div>\n\n\n### Decomposition\n\nWe actually want to take county fixed effects into account. We therefore want to decompose this distribution into a between-counties component and a within-county one.\n\nWe first assume that de data generating process is as follows:\n\n1. For each county, draw observations for all the years from a distribution shared across counties ($WC$)\n2. Add county fixed-effects that shift the mean of this distribution differently for each county. These fixed-effects therefore represent the variation in means that are county specific.\n\nIn other words, we assume that the DGP is as follows for $logyield$ in county $c$ in year $t$:\n\n$$logyield_{ct} = \\alpha + WC_t + BC_c$$\nWhere $WC_t \\sim f_t$ is a distribution shared across counties and $BC_c \\sim g_c$ is a distribution shared across years. Both distribution have mean 0 ; the overall mean is captured by $\\alpha$.\n\nThe goal is to retrieve the two distributions and the constant. The DAG for our assumed DGP here is:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](sim_2_high_temp_questions_files/figure-html/DAG-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### Distribution of within-county log corn yields\n\nLet's therefore first demean the (log)yields by removing the county average (log)yields. This gives a distribution of (log)yields centered on 0. Plot this distribution and overlay a (calibrated) theoretical distribution on it.\n\n::: callout-note\nWe need to filter out a few observations for which logcornyield is equal to -Infinity (use `filter(logcornyield != -Inf)`) \n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#plot\n```\n:::\n\n\n\n### Distribution of between-counties log corn yields\n\nLet's then explore how the \"fixed-effects\" are distributed. Plot the distribution of the fixed effects (*ie* of the county means) and overlay a theoretical distribution on it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#plot\n```\n:::\n\n\n\n## Distribution of `higher`\n\nWe then turn to the explanatory variable of interest, `higher`. This variable represents the number of GDDs above 28°C.\n\nWe start by plotting this distribution and compute its mean and standard deviation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot\n```\n:::\n\n\nWe then reproduce the same analysis as for log yields. We assume that \"higher GDD\" in each counties are drawn from the same underlying distribution but shifted by a fixed effect. We therefore first want to compute the shape of the distribution of the demeaned `higher` variables.\n\nFirst, we plot the distribution of the demeaned version of the variable and overlay a theoretical distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#plot\n```\n:::\n\n\nWe then explore the distribution of the fixed-effects. Plot the distribution of the \"fixed-effects\".\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#plot\n```\n:::\n\n\nWhich type of distribution could roughly fit this shape? <select class='webex-select'><option value='blank'></option><option value=''>Normal</option><option value=''>Uniform</option><option value='answer'>Gamma</option><option value=''>Bernoulli</option><option value=''>Poisson</option></select>.\n\nWe will want to overlay a theoretical distribution to this graph.\n\n\n<div class='webex-solution'><button>Click here only after you have answered the previous question</button>\n\nWe derive rough parameters values from properties of the gamma distribution and then adjust them manually. Denoting respectively $k$ and $\\theta$ the shape and scale parameters, the mean of the distribution is given by $k \\theta$ and the variance by $k \\theta^2$. We thus solved the system such that mean $\\simeq 68 \\ (= k \\theta)$ and var $\\simeq 59  \\ (= k \\theta^2)$.\n\nWe therefore assume that the fixed-effects are drawn from a gamma distribution with shape 1.3, scale 50. \n\nNote that, we want a distribution centered on 0 so we need to shift the distribution by its mean. The theoretical function to plot is therefore:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshape_g <- 1.3\nscale_g <- 50\n\nfun_gamma <-  \\(x) dgamma(x + shape_g*scale_g, shape = shape_g, scale = scale_g)\n```\n:::\n\n\n</div>\n\n\n\n# Simulation\n\n::: {.callout-tip title=\"Objective of the Simulation\"}\nThrough this simulation, we will explore whether the model we write actually retrieves the effects we are interested in, in a pristine setting.\n:::\n\n## Modeling choices\n\nBased on the exploration of the data, we assume that de data generating process for both yield and high degree days is as follows:\n\n1. Observations are drawn for all the years from a distribution shared across counties\n2. These distributions are \"shifted\" county by county by adding county fixed effects---drawn from a distribution with mean 0, leaving the average unchanged---.\n\nThis leads to the following equation:\n\n$$logy_{ct} = \\beta_0 + \\beta_1 H_{ct} + \\lambda_c + u_{ct}$$\n\nwhere $logy_{ct}$ is the log of the average crop yield in county $c$ in year $t$, $H_{ct}$ the number of GDDs above a given threshold (28°C), $\\lambda_c$ county fixed effects and $u_{ct}$ and error term. (log-)yields can be interpreted as the sum of:\n\n1. A draw from a shared distribution (with variance $\\sigma_u^2$) whose mean is county dependent ($\\beta_0 + \\lambda_c$)\n1. An effect of high degree days ($H_{ct}$), affecting the variance and mean the of log-yields.\n\nFurther, we assume that high degree days are defined as follows:\n\n$$H_{ct} = \\mu + \\eta_c + \\epsilon_{ct}$$\n\nWhere $\\eta_c$ are county fixed effects, $\\epsilon_{ct}$ an error term and $\\mu$ an intercept. As for yields, $H$ in a given year are drawn from a shared distribution (represented by $\\epsilon_{ct}$) whose mean is county dependent ($\\mu + \\eta_c$). For simplicity in the simulations, we distinguish the county fixed effects for $H$ and $yield$ since the invariant component for each variable does not have same distribution. We will therefore generate two distinct fixed effect variables. Regardless, they are partialled out with the introduction of fixed effects.\n\n\nMore precisely, let's set, mostly based on our previous analysis exploring the shape of the distributions:\n\n- $N_c$ the number of counties\n- $N_t$ the number of time periods\n- $\\eta_c \\sim \\text{Gamma}(k, \\theta) - k\\theta$ county fixed effects for $H$. Withdrawing $k\\theta$, the mean of the gamma distribution, ensures that in expectation these fixed effects are equal to 0. For readability, we call $k$ `shape_fe_h` and $\\theta$ `scale_fe_h`.\n- $\\epsilon_{ct} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$ the shared distribution component of $H_{ct}$. \n- $H_{ct} = \\mu_{H} + \\eta_c + \\epsilon_{ct}$\n- $\\lambda_c \\sim \\mathcal{N}(0, \\sigma_{\\lambda}^2)$ county fixed effects for $logy$. \n<!-- - $FE_t = \\lambda_t \\sim  \\mathcal{N}(0, \\sigma_{\\lambda}^{2})$ -->\n- $u_{ct} \\sim \\mathcal{N}(0, \\sigma_{u}^{2})$\n- $logy_{ct} = \\beta_0 + \\beta_1 H_{ct} + \\lambda_c + u_{ct}$\n\n## Generate data\n\nWrite a `generate_data` function to generate data according to this DGP. Note that you will need to generate a panel. To do that, you can use the `crossing` function.\n\n## Calibration and baseline parameters' values\n\nBased on the data exploration, explain which baseline parameters you would choose for the following variables:\n\n- **Number of observations**: \n- **Distribution of the variables**: \n- **Treatment effect**:\n\nCreate a `baseline_param` dataset and a `generate_data()` function. \n\n## Data exploration\n\nWe can now quickly explore the data we generated. Verify that the distribution are correct, *ie* that the calibration has been done properly. Plot the distributions of the variables and compute their mean and standard deviation.\n\n## Model estimation\n\nWhich model would you estimate? \n\nWrite a function to run the estimation. \n\nThen, write a function that generates a dataset and runs the estimation on this data set. Do you recover the effect of interest?\n\n## All simulations\n\nRepeat the analysis several times, without varying the parameters. Do you retrieve the true effect you put in the data? \n\nIf not, explore why and fix the issue. \n\n## Complexify the DGP\n\nThen, complexify the DGP by adding some correlation\nbut do not change the econometric model. The goal is to explore what happens if the econometric model does not represent the DGP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "sim_2_high_temp_questions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}