{
  "hash": "33f42a3828eeccd864bc0309f04f1f9d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Replication and Calibration\"\nsubtitle: \"Replication of a paper on impacts of high temperature on agricultural yields.\"\npublished-title: \"Date\"\ndate: \"2024-10-09\"\nself-contained: true\nengine: knitr\nhtml-math-method: mathml\n---\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip title=\"Objective\"}\nReplicate and simulate a paper to allow us to evaluate the performance and robustness of the analysis.\n:::\n\nWe first load the packages used in this document.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code to load the R packages used in this doc\"}\nlibrary(tidyverse)\nlibrary(knitr) \nlibrary(kableExtra)\nlibrary(broom) \nlibrary(here)\nlibrary(fixest) #for fixed effect regression\nlibrary(haven) #to read Stata .dta files\nlibrary(modelsummary)\n\nset.seed(35) #fix the starting point of the random numbers generator\n\n# The code below is to use my own package for ggplot themes\n# You can remove it. Otherwise you will need to install the package via GitHub\n# using the following line of code\n# devtools::install_github(\"vincentbagilet/mediocrethemes\")\nlibrary(mediocrethemes)\nset_mediocre_all(pal = \"portal\") #set theme for all the subsequent plots\n\n# For exercises\nlibrary(webexercises)\n```\n:::\n\n\n# Setting\n\nIn this example, we explore the **impact of climate change (or high temperatures) on agricultural yields**. Usual approaches to explore this question rely on variation over time to compare hot and cold conditions for a given area. This approach has been implemented in fear of omitted variable bias in traditional cross-sectional approaches (comparing hot areas to cold areas), climate being potentially correlated with other time invariant unobserved variables. \nA large set of papers leverage this approach to address this question, *e.g.* [Deschênes and Greenstone (2007)](https://www.aeaweb.org/articles?id=10.1257/aer.97.1.354), [Schlenker and Roberts (2009)](https://www.pnas.org/doi/10.1073/pnas.0906865106) or [Burke and Emerick (2016)](https://www.aeaweb.org/articles?id=10.1257/pol.20130025).\n\nIn the present analysis, we will build simulations to emulate a typical study from this literature, measuring the impact of high temperatures on agricultural yields. A typical approach consists in estimating the link between high Growing Degree Days (GDD, roughly the number of days a crop is exposed to a certain range of temperatures) and yields, controlling for characteristics that are invariant in time and across locations. As in Burke and Emerick (2016) and some specifications from Schlenker and Roberts (2009), we will model log corn yield as a piecewise linear function of temperature, considering high and low GDDs. Most of the analyses in the literature being at the county-year level, we will simulate data with a similar structure.\n\nTo explore the data generating process, we will leverage data from [Burke and Emerick (2016)](https://www.aeaweb.org/articles?id=10.1257/pol.20130025), henceforth BE. This will allow us to make a few simplifying assumptions regarding the data generating process. By replicating BE, we will try to confirm that these assumptions do not significantly alter the results of the estimation in an actual study and that simulations under these assumptions remain close to an actual study from this literature. \n\nLet's thus first replicate the analysis in BE.\n\n# Replication\n\n## Replication setting and set up\n\n### Exploring the results\n\n[Burke and Emerick (2016)](https://www.aeaweb.org/articles?id=10.1257/pol.20130025), henceforth BE, aims to evaluate the impact of high temperature days on corn production, estimating the following equation:\n\n$$\nlog(y_{ct}) = \\beta_0 + \\beta_1 H_{ct} + \\beta_2 L_{ct} + \\beta_3 H^{prec}_{ct} + \\beta_4 L^{prec}_{ct} + \\lambda_c + \\kappa_t + u_{ct}\n$$ {#eq-main-model}\n\nwhere $y_{ct}$ is the average crop yield in county $c$ in year $t$, $L_{ct}$ the number of Growing Degree Days (GDDs)^[BE presents it as \"the amount of time a crop is exposed to temperatures between a given lower and upper bound\". [Schlenker and Roberts (2009)](https://www.pnas.org/doi/10.1073/pnas.0906865106) gives an example: for \"bounds of 8°C and 32°C [...] a day of 9°C contributes 1 degree day, a day of 10°C contributes 2 degree days, up to a temperature of 32°C, which contributes 24 degree days. All temperatures above 32°C also contribute 24 degree days. Degree days are then summed over the entire season.”] below a given threshold (28 and 29°C for the panel and long-differences models respectively), $H$ the number of GDDs above this threshold. $H^{prec}_{ct}$ and $L^{prec}_{ct}$ represent equivalent measures for precipitations (threshold = 42 cm/year for the panel model, 50 for the long-differences one). $\\lambda_c$ and $\\kappa_t$ are county and time fixed effects respectively and $u_{ct}$ an error term. \n\nWhat is the parameter of interest here? <input class='webex-solveme nospaces' size='6' data-answer='[\"beta 1\",\"beta_1\",\"beta1\"]'/>\n\nThe results we want to replicate are displayed in the paper, in Table 1 column 3:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](data/replication_BE/table1_paper.png){fig-align='center' width=100%}\n:::\n:::\n\n\nWhat is their estimated value of the parameter of interest? <input class='webex-solveme nospaces' data-tol='5e-04' size='7' data-answer='[\"-0.0056\",\"-.0056\"]'/>\n\nIs their design good enough to have a high probability (let's say more than 80%) to detect the true effect if it was equal to half the size of the estimated effect? \n\nTo explore this we can use the function <input class='webex-solveme nospaces' size='12' data-answer='[\"retrodesing\",\"retro_design\"]'/>. \n\nThis function takes two parameters: the hypothetical true effect size (here <input class='webex-solveme nospaces' data-tol='5e-04' size='7' data-answer='[\"-0.0028\",\"-.0028\"]'/>) and the <input class='webex-solveme nospaces' size='14' data-answer='[\"se\",\"standard error\"]'/> of the obtained estimate. Run this function. What would be the power of this analysis? <input class='webex-solveme nospaces' data-tol='1' size='3' data-answer='[\"100\"]'/>%. Does this analysis seem powered enough to detect an effect of this magnitude? <select class='webex-select'><option value='blank'></option><option value='answer'>Yes</option><option value=''>No</option></select>.\n\n::: callout-note\nIdeally, we would also test whether the design is good enough to detect effects that are consistent with typical effect sizes from the literature. For instance if a typical effect size in the literature was -0.001, would this design be good enough to detect it?\n:::\n\nTry several hypothetical effect sizes. Note that you can pass a vector of hypothetical true effect sizes to the function. You can then plot a graph of the statistical power as a function of the hypothetical effect size for instance.\n\nFor which hypothetical true effect size does the statistical power become smaller than 80%? <input class='webex-solveme nospaces' data-tol='0.001' size='6' data-answer='[\"-0.002\",\"-.002\"]'/>\n\n::: {.callout-tip title=\"Objective of the analysis\"}\nThe goal of the present analysis will be to assess whether the design in BE is good enough to detect an effect size equal to half the effect size of the estimate found in the analysis.\n\nNote that here we are replicating an analysis that has already been implemented. Therefore, we can run a retro design calculation even before doing anything else and in particular, before implementing the simulation. If we were implementing our own analysis, we could not do that and would only be able to run this retro design analysis after completing the whole analysis, hence the use of the simulation. In the present setting the simulation may seem useless as we know what result it should yield. We could however use the present simulation to study the drivers of statistical power for instance, or test for the effect of potentially erroneous hypotheses (here we are going to estimate an econometric model that exactly fits the DGP. We may wonder what would happen if it was not the case?)\n:::\n\n### Data wrangling and cleaning\n\nTheir code is written in `Stata`; we can translate it to `R`. For simplicity and since it is not the purpose of this exercise, I already did that and cleaned the data.\n\nYou however need to download their [replication package](http://doi.org/10.3886/E114567V1) on [the paper's page](https://www.aeaweb.org/articles?id=10.1257/pol.20130025). Then store the data set `us_panel.dta` in your working directory (or a sub directory).\n\n::: callout-warning\nYou will need to change the path in `read_dta()` and adapt it to your case.\n:::\n\n\n::: {.cell layout-align=\"center\" code_folding='true'}\n\n```{.r .cell-code}\nBE_data <- haven::read_dta(\n  here::here(\"content\", \"sim\", \"data\", \"replication_BE\", \"us_panel.dta\"))\n\nBE_wrangled <- BE_data |>\n  filter(year >= 1978 & year <= 2002) |> \n  filter(longitude > -100) |> \n  transmute(\n    year,\n    fips = str_pad(fips, 5, \"left\", \"0\"),\n    stfips = str_sub(fips, 1, 2),\n    cornyield,\n    logcornyield = log(cornyield),\n    lower = dday0C - dday29C,\n    higher = dday29C,\n    prec_lo = ifelse(prec <= 42, prec - 42, 0),\n    prec_hi = ifelse(prec > 42, prec - 42, 0),\n    corn_area\n  ) \n\nBE_clean <- BE_wrangled |> \n  group_by(fips) |> \n  mutate(\n    #/sum to compute the equivalent of analytic weights in stata\n    corn_area_78_02 = mean(corn_area, na.rm = TRUE)/sum(!is.na(corn_area))\n  ) |> \n  ungroup()\n```\n:::\n\n\n## Data structure\n\nBefore anything, let's look at this data. It is a panel with a time dimension, <input class='webex-solveme nospaces' size='4' data-answer='[\"year\"]'/> and an individual dimension, <input class='webex-solveme nospaces' size='4' data-answer='[\"fips\"]'/>. \n\nHow many time periods are there in this panel? <input class='webex-solveme nospaces' size='2' data-answer='[\"25\"]'/> \n\nAnd how many individuals (counties)?  <input class='webex-solveme nospaces' size='4' data-answer='[\"2459\"]'/>\n\nIs this panel balanced? <select class='webex-select'><option value='blank'></option><option value=''>Yes</option><option value=''>Not at all</option><option value='answer'>Almost</option></select>.\n\n\n\n\n\nThe standard errors are clustered at the state level, we first have a quick look at this additional layer in the panel. How many states are there in this panel? <input class='webex-solveme nospaces' size='2' data-answer='[\"37\"]'/>. Is it equal to the number of states in the US? Why is that?\n\nHow many observations are there on average by state? (county/years) <input class='webex-solveme nospaces' data-tol='1' size='16' data-answer='[\"1659.64864864865\"]'/>. Is the number of observations per state uniformely distributed across states? <select class='webex-select'><option value='blank'></option><option value=''>Yes</option><option value='answer'>Not at all</option><option value=''>Almost</option></select> \n\n## Actual replication\n\nBefore anything, let's try to replicate the main regression, as specified in the paper. The model is the one described in @eq-main-model. \n\nSince the regression model specified in the paper is a fixed effect one, we are going to estimate it using the `feols()` function from the `fixest` package. Look at the documentation for this function. To access it, you can use the <input class='webex-solveme nospaces' size='6' data-answer='[\"?\",\"help\",\"help()\"]'/> function. \n\n::: callout-warning\nTo be able to replicate the results in the analysis, we need to include weights and cluster the standard errors at the state level.\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_fe <- feols(\n  data = BE_clean,\n  logcornyield ~ lower + higher + prec_lo + prec_hi | as_factor(year) + fips,\n  weights = ~ corn_area_78_02,\n  cluster = ~ stfips\n)\n```\n:::\n\n\n## A simpler model?\n\nWe will generate data assuming that the econometric model we want to estimate accurately represents the DGP. However, this econometrics model is rather complex and it might be challenging to directly simulate a DGP based on it. Let's try to see if a simpler model would yield very different results.\n\nRemove the controls (*ie* only keeping one explanatory variable) and time fixed effects from your econometric model. Estimate this new econometric model. The explained variable is <input class='webex-solveme nospaces' size='12' data-answer='[\"logcornyield\"]'/>. **The** explanatory variable is <input class='webex-solveme nospaces' size='6' data-answer='[\"higher\"]'/> and the fixed effects are <input class='webex-solveme nospaces' size='4' data-answer='[\"fips\"]'/>. Let's also remove the weights for simplicity.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg_fe_no_ctrl <- feols(\n  data = BE_clean,\n  logcornyield ~ higher | fips,\n  cluster = ~stfips\n)\n\nlist(reg_fe, reg_fe_no_ctrl) |> \n  modelsummary::modelsummary(gof_omit = \"IC|Log|R\",  fmt = 4)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n      function styleCell_woq583mwfezgc10cgb22(i, j, css_id) {\n        var table = document.getElementById(\"tinytable_woq583mwfezgc10cgb22\");\n        table.rows[i].cells[j].classList.add(css_id);\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_woq583mwfezgc10cgb22');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_woq583mwfezgc10cgb22(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_woq583mwfezgc10cgb22\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(0, 0, 'tinytable_css_idvjpr8njq5xnkb653jvk7') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(0, 1, 'tinytable_css_idx9xjbwnvwgn1de823fgr') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(0, 2, 'tinytable_css_idx9xjbwnvwgn1de823fgr') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(1, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(1, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(1, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(2, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(2, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(2, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(3, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(3, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(3, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(4, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(4, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(4, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(5, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(5, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(5, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(6, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(6, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(6, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(7, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(7, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(7, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(8, 0, 'tinytable_css_id5fspvz2g6qdev1jw0br7') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(8, 1, 'tinytable_css_idq6s9aq0s8lg7px19ndgw') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(8, 2, 'tinytable_css_idq6s9aq0s8lg7px19ndgw') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(9, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(9, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(9, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(10, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(10, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(10, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(11, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(11, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(11, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(12, 0, 'tinytable_css_idyg2m55b6cxnltr1uicem') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(12, 1, 'tinytable_css_idjyw38d9unwn863h8nisp') })\nwindow.addEventListener('load', function () { styleCell_woq583mwfezgc10cgb22(12, 2, 'tinytable_css_idjyw38d9unwn863h8nisp') })\n    </script>\n\n    <style>\n    .table td.tinytable_css_idvjpr8njq5xnkb653jvk7, .table th.tinytable_css_idvjpr8njq5xnkb653jvk7 {  text-align: left;  border-bottom: solid 0.1em #d3d8dc; }\n    .table td.tinytable_css_idx9xjbwnvwgn1de823fgr, .table th.tinytable_css_idx9xjbwnvwgn1de823fgr {  text-align: center;  border-bottom: solid 0.1em #d3d8dc; }\n    .table td.tinytable_css_idyg2m55b6cxnltr1uicem, .table th.tinytable_css_idyg2m55b6cxnltr1uicem {  text-align: left; }\n    .table td.tinytable_css_idjyw38d9unwn863h8nisp, .table th.tinytable_css_idjyw38d9unwn863h8nisp {  text-align: center; }\n    .table td.tinytable_css_id5fspvz2g6qdev1jw0br7, .table th.tinytable_css_id5fspvz2g6qdev1jw0br7 {  border-bottom: solid 0.05em black;  text-align: left; }\n    .table td.tinytable_css_idq6s9aq0s8lg7px19ndgw, .table th.tinytable_css_idq6s9aq0s8lg7px19ndgw {  border-bottom: solid 0.05em black;  text-align: center; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_woq583mwfezgc10cgb22\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">(1)</th>\n                <th scope=\"col\">(2)</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>lower              </td>\n                  <td>0.0004    </td>\n                  <td>           </td>\n                </tr>\n                <tr>\n                  <td>                   </td>\n                  <td>(0.0001)  </td>\n                  <td>           </td>\n                </tr>\n                <tr>\n                  <td>higher             </td>\n                  <td>-0.0055   </td>\n                  <td>-0.0056    </td>\n                </tr>\n                <tr>\n                  <td>                   </td>\n                  <td>(0.0006)  </td>\n                  <td>(5.969e-04)</td>\n                </tr>\n                <tr>\n                  <td>prec_lo            </td>\n                  <td>0.0118    </td>\n                  <td>           </td>\n                </tr>\n                <tr>\n                  <td>                   </td>\n                  <td>(0.0027)  </td>\n                  <td>           </td>\n                </tr>\n                <tr>\n                  <td>prec_hi            </td>\n                  <td>-0.0007   </td>\n                  <td>           </td>\n                </tr>\n                <tr>\n                  <td>                   </td>\n                  <td>(0.0005)  </td>\n                  <td>           </td>\n                </tr>\n                <tr>\n                  <td>Num.Obs.           </td>\n                  <td>48465     </td>\n                  <td>48465      </td>\n                </tr>\n                <tr>\n                  <td>Std.Errors         </td>\n                  <td>by: stfips</td>\n                  <td>by: stfips </td>\n                </tr>\n                <tr>\n                  <td>FE: fips           </td>\n                  <td>X         </td>\n                  <td>X          </td>\n                </tr>\n                <tr>\n                  <td>FE: as_factor(year)</td>\n                  <td>X         </td>\n                  <td>           </td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\nAre the results approximately comparable to the full model? <select class='webex-select'><option value='blank'></option><option value='answer'>Yes, basically</option><option value=''>Not at all</option></select> \n\nWe will therefore simulate this simplified model. How many variables do we need to generate? <input class='webex-solveme nospaces' size='1' data-answer='[\"4\"]'/>\n\n\n\n<!-- Based on these results, and compared to the literature, we therefore abstract from modeling: -->\n\n<!-- - Precipitations, to avoid having to simulate complex interactions with temperature.  -->\n<!-- - Time fixed effects. We only model county fixed effects in order to have only one varying parameter. However, we take time fixed effects into account: the distributions of the variables represent those of the observed variables *after removing time fixed effects*. This is equivalent to working on a version of the data in which these time fixed effects have already been partialled out.  -->\n<!-- - Growing degree days below the threshold, to simplify the data generating process, only considering those above ($HighGDD$), the variable of interest -->\n<!-- - Correlation between counties. While this makes the simulations less representative of real cases, it mostly improves and facilitates the estimation. We could complexify our simulations to include this correlation in a later stage. -->\n\n\n\n\n# Distribution of the variables\n\nTo be able to generate a fake data set, we first need to explore the distribution of  the variables.\n\n## Distribution the logcornyield\n\nWe first focus on the log of corn yields. Since the dependent variable in the analysis is the transformed version of corn yields, we will directly generate it, instead of generating cornyield and then taking its log. We start by plotting the distribution of log corn yields and fitting a normal to it, manually selecting the parameters of the normal distribution.\n\nLet's first plot the distribution of `logcornyield`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngraph_check_distrib <- function(data, var, mean_norm, sd_norm) {\n  data |> \n    ggplot(aes(x = {{ var }})) + \n    geom_density() +\n    geom_function(\n      fun = \\(x) dnorm(x, mean  = mean_norm, sd = sd_norm),\n      linewidth = 1\n    ) + #manually tune the params\n    labs(\n      y = \"Density\",\n      caption = str_c(\"Mean normal: \", mean_norm, \", sd: \", sd_norm)\n    )\n}\n\nBE_clean |> \n  graph_check_distrib(logcornyield, 4.55, 0.33) +\n  labs(\n    title = \"Distribution of corn yields\",\n    subtitle = \"Tentative normal fit\",\n    x = \"Corn yield\"\n  )\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/distrib_logyield_sol-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nWhat could be a decent approximation of this distribution ? Using `geom_function()`, let's overlay the cdf of this theoretical distribution on top of the distribution of logcornyield. Manually adjust the parameters of the theoretical distribution to get a good fit.\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nBE_clean |> \n  ggplot(aes(x = logcornyield)) + \n  geom_density() + \n  geom_function(\n    fun = \\(x) dnorm(x, mean  = 4.55, sd = 0.33),\n      linewidth = 1\n    ) +\n  labs(\n    title = \"Distribution of log corn yields\",\n    subtitle = \"Tentative normal fit\",\n    x = \"Log Corn Yield\",\n    y = \"Denisty\"\n  )\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/distrib_logcornyield_fit-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n</div>\n\n\n### Decomposition\n\nWe actually want to take county fixed effects into account. We therefore want to decompose this distribution into a between-counties component and a within-county one.\n\nWe first assume that de data generating process is as follows:\n\n1. For each county, draw observations for all the years from a distribution shared across counties ($WC$)\n2. Add county fixed-effects that shift the mean of this distribution differently for each county. These fixed-effects therefore represent the variation in means that are county specific.\n\nIn other words, we assume that the DGP is as follows for $logyield$ in county $c$ in year $t$:\n\n$$logyield_{ct} = \\alpha + WC_t + BC_c$$\nWhere $WC_t \\sim f_t$ is a distribution shared across counties and $BC_c \\sim g_c$ is a distribution shared across years. Both distribution have mean 0 ; the overall mean is captured by $\\alpha$.\n\nThe goal is to retrieve the two distributions and the constant. The DAG for our assumed DGP here is:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/DAG-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nTo further explain the between/within county decomposition, let's plot a graph:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nsample_counties <- sample(unique(BE_clean$fips) , 15)\n\nBE_clean |> \n  filter(cornyield > 0) |> \n  filter(fips %in% sample_counties) |>\n  ggplot(aes(x = logcornyield, y = fips)) + \n  ggridges::geom_density_ridges(\n    fill = colors_mediocre[[\"base\"]],\n    color = colors_mediocre[[\"base\"]],\n    alpha = 0.5\n  ) +\n  labs(\n    title = \"Distribution of Log Corn Yield by County\",\n    subtitle = \"For a random subset of counties\",\n    # subtitle = paste(\"In the\", fips, \"county\"),\n    x = \"Log cornyield\",\n    y = \"County identifyier\"\n  ) \n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/ridge_logcornyield-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nThis graph represents, for a subset of counties, the distribution, by county, of log corn yields for every year in the sample. The idea is that logcornyield come from an identical distribution for each county, the only difference being that it is shifted by a certain quantity (the fixed effect) for each county. The fixed-effect for a county therefore represent the mean of logcornyield for this county (more precisely the distance of this mean to the overall mean of logcornyield).\n\n::: callout-note\nYou might (rightly) think that the distribution is absolutely not the same for each county. However, remember that we observe only 25 years of data. Each distribution is thus only composed of 25 points. Even if the underlying distribution was indeed the same, repeated density plots of 25 draws from this distribution would look very different. To convince yourself of that, you can for instance run the following lines of code several times:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = rnorm(25)) |> \n  ggplot(aes(x = x)) + \n  geom_density() + \n  xlim(c(-5, 5))\n```\n:::\n\n\nThis however does not mean that the hypothesis of an identical within-county distribution holds. But let's assume it does, for the sake of the exercise. To double check that, we would need much more observations per group. To do so, instead of plotting distributions by county, we could for instance plot them by groups of counties. \n:::\n\n### Distribution of within-county log corn yields\n\nLet's therefore first demean the (log)yields by removing the county average (log)yields. This gives a distribution of (log)yields centered on 0. Plot this distribution and overlay a (calibrated) theoretical distribution on it.\n\n::: callout-note\nWe need to filter out a few observations for which logcornyield is equal to -Infinity (use `filter(logcornyield != -Inf)`) \n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nBE_demean_logyield <- BE_clean |>\n  filter(logcornyield != -Inf) |> #100 obs. To avoid pbs when computing mean\n  group_by(fips) |> \n  mutate(\n    county_mean_logyield = mean(logcornyield, na.rm = TRUE)\n  ) |> \n  ungroup() %>%\n  mutate(\n    logyield_demean = (logcornyield - county_mean_logyield) + \n      mean(.$county_mean_logyield, na.rm = TRUE)\n  )\n\nBE_demean_logyield |> \n  graph_check_distrib(logyield_demean, 4.55, 0.2) + #manually tune the params\n  labs(\n    title = \"Distribution of log of corn yields\",\n    subtitle = \"Removing between counties mean variation\\nand tentative normal fit\",\n    x = \"Demeaned logcornyield\"\n  )\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/distrib_within_logcornyield_sol-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n### Distribution of between-counties log corn yields\n\nLet's then explore how the \"fixed-effects\" are distributed. Plot the distribution of the fixed effects (*ie* of the county means) and overlay a theoretical distribution on it.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nBE_demean_logyield %>%\n  mutate(\n    county_fe = county_mean_logyield - mean(.$county_mean_logyield, na.rm = TRUE)\n  ) |> \n  graph_check_distrib(county_fe, 0, 0.26) + \n  labs(\n    title = 'Distribution of county \"fixed-effects\"',\n    subtitle = \"Tentative normal fit\",\n    x = \"Distance of county mean to the overall mean\"\n  )\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/distrib_fe_yield_sol-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n## Distribution of `higher`\n\nWe then turn to the explanatory variable of interest, `higher`. This variable represents the number of GDDs above 28°C.\n\nWe start by plotting this distribution and compute its mean and standard deviation.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nBE_clean %>% {\n  ggplot(., aes(x = higher)) + \n  geom_histogram(bins = 100) + \n  labs(\n    title = \"Distribution of 'higher'\",\n    x = \"Higher\",\n    y = \"Count\",\n    caption = paste(\n      \"Mean =\", round(mean(.$higher, na.rm = TRUE), 1),\n      \", sd =\", round(sd(.$higher, na.rm = TRUE), 1)\n    )\n  )\n}\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/distrib_higher_sol-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nWe then reproduce the same analysis as for log yields. We assume that \"higher GDD\" in each counties are drawn from the same underlying distribution but shifted by a fixed effect. We therefore first want to compute the shape of the distribution of the demeaned `higher` variables.\n\nFirst, we plot the distribution of the demeaned version of the variable and overlay a theoretical distribution.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndemean_higher <- BE_clean |>\n  group_by(fips) |> \n  mutate(\n    county_mean_higher = mean(higher, na.rm = TRUE),\n    sd_higher = sd(higher, na.rm = TRUE)\n  ) |> \n  ungroup() %>%\n  mutate(\n    higher_demean = (higher - county_mean_higher) + \n      mean(.$county_mean_higher, na.rm = TRUE)\n  )\n\ndemean_higher |> \n  graph_check_distrib(higher_demean, 65, 18) +\n  labs(\n    title = \"Distribution of the 'higher', centered by county\",\n    subtitle = \"Very tentative normal fit\",\n    x = \"County mean of 'higher'\"\n  )\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/distrib_higher_demeaned_sol-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nWe then explore the distribution of the fixed-effects. Plot the distribution of the \"fixed-effects\".\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# shift_g <- 68\n# shape_g <- 1.3\n# scale_g <- 50\n\ndemean_higher %>%\n  mutate(\n    county_fe = county_mean_higher - mean(.$county_mean_higher, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = county_fe)) + \n  geom_density() +\n  labs(\n    title = \"Distribution of the county means of 'higher'\",\n    x = \"Distance of county mean of `higher` to the overall mean\",\n    y = \"Density\"\n  )\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/distribe_fe_higher_sol-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nWhich type of distribution could roughly fit this shape? <select class='webex-select'><option value='blank'></option><option value=''>Normal</option><option value=''>Uniform</option><option value='answer'>Gamma</option><option value=''>Bernoulli</option><option value=''>Poisson</option></select>.\n\nWe will want to overlay a theoretical distribution to this graph.\n\n\n<div class='webex-solution'><button>Click here only after you have answered the previous question</button>\n\nWe derive rough parameters values from properties of the gamma distribution and then adjust them manually. Denoting respectively $k$ and $\\theta$ the shape and scale parameters, the mean of the distribution is given by $k \\theta$ and the variance by $k \\theta^2$. We thus solved the system such that mean $\\simeq 68 \\ (= k \\theta)$ and var $\\simeq 59  \\ (= k \\theta^2)$.\n\nWe therefore assume that the fixed-effects are drawn from a gamma distribution with shape 1.3, scale 50. \n\nNote that, we want a distribution centered on 0 so we need to shift the distribution by its mean. The theoretical function to plot is therefore:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshape_g <- 1.3\nscale_g <- 50\n\nfun_gamma <-  \\(x) dgamma(x + shape_g*scale_g, shape = shape_g, scale = scale_g)\n```\n:::\n\n\n</div>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nshift_g <- 68\nshape_g <- 1.3\nscale_g <- 50\n\ndemean_higher %>%\n  mutate(\n    county_fe = county_mean_higher - mean(.$county_mean_higher, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = county_fe)) + \n  geom_density() +\n  geom_function(\n    fun = \\(x) dgamma(x + shape_g*scale_g, shape = shape_g, scale = scale_g),\n    linewidth = 1\n  ) + #manually tune the param based on the values above\n  labs(\n    title = \"Distribution of the county means of 'higher'\",\n    subtitle = \"Tentative gamma fit\",\n    x = \"Distance of county mean of `higher` to the overall mean\",\n    y = \"Density\",\n    caption = str_c(\n      \"Param of the gamma distrib: shape = \", \n      shape_g, \", scale = \", scale_g,\n      \". Shifted by \", shift_g)\n  )\n```\n\n::: {.cell-output-display}\n![](sim_2_high_temp_files/figure-html/distribe_fe_higher_fit-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n# Simulation\n\n::: {.callout-tip title=\"Objective of the Simulation\"}\nThrough this simulation, we will explore whether the model we write actually retrieves the effects we are interested in, in a pristine setting.\n:::\n\n## Modeling choices\n\nBased on the exploration of the data, we assume that de data generating process for both yield and high degree days is as follows:\n\n1. Observations are drawn for all the years from a distribution shared across counties\n2. These distributions are \"shifted\" county by county by adding county fixed effects---drawn from a distribution with mean 0, leaving the average unchanged---.\n\nThis leads to the following equation:\n\n$$logy_{ct} = \\beta_0 + \\beta_1 H_{ct} + \\lambda_c + u_{ct}$$\n\nwhere $logy_{ct}$ is the log of the average crop yield in county $c$ in year $t$, $H_{ct}$ the number of GDDs above a given threshold (28°C), $\\lambda_c$ county fixed effects and $u_{ct}$ and error term. (log-)yields can be interpreted as the sum of:\n\n1. A draw from a shared distribution (with variance $\\sigma_u^2$) whose mean is county dependent ($\\beta_0 + \\lambda_c$)\n1. An effect of high degree days ($H_{ct}$), affecting the variance and mean the of log-yields.\n\nFurther, we assume that high degree days are defined as follows:\n\n$$H_{ct} = \\mu + \\eta_c + \\epsilon_{ct}$$\n\nWhere $\\eta_c$ are county fixed effects, $\\epsilon_{ct}$ an error term and $\\mu$ an intercept. As for yields, $H$ in a given year are drawn from a shared distribution (represented by $\\epsilon_{ct}$) whose mean is county dependent ($\\mu + \\eta_c$). For simplicity in the simulations, we distinguish the county fixed effects for $H$ and $yield$ since the invariant component for each variable does not have same distribution. We will therefore generate two distinct fixed effect variables. Regardless, they are partialled out with the introduction of fixed effects.\n\n\nMore precisely, let's set, mostly based on our previous analysis exploring the shape of the distributions:\n\n- $N_c$ the number of counties\n- $N_t$ the number of time periods\n- $\\eta_c \\sim \\text{Gamma}(k, \\theta) - k\\theta$ county fixed effects for $H$. Withdrawing $k\\theta$, the mean of the gamma distribution, ensures that in expectation these fixed effects are equal to 0. For readability, we call $k$ `shape_fe_h` and $\\theta$ `scale_fe_h`.\n- $\\epsilon_{ct} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$ the shared distribution component of $H_{ct}$. \n- $H_{ct} = \\mu_{H} + \\eta_c + \\epsilon_{ct}$\n- $\\lambda_c \\sim \\mathcal{N}(0, \\sigma_{\\lambda}^2)$ county fixed effects for $logy$. \n<!-- - $FE_t = \\lambda_t \\sim  \\mathcal{N}(0, \\sigma_{\\lambda}^{2})$ -->\n- $u_{ct} \\sim \\mathcal{N}(0, \\sigma_{u}^{2})$\n- $logy_{ct} = \\beta_0 + \\beta_1 H_{ct} + \\lambda_c + u_{ct}$\n\n## Generate data\n\nWrite a `generate_data` function to generate data according to this DGP. Note that you will need to generate a panel. To do that, you can use the `crossing` function.\n\n## Calibration and baseline parameters' values\n\nBased on the data exploration, explain which baseline parameters you would choose for the following variables:\n\n- **Number of observations**: [Deschênes and Greenstone (2007)](https://www.aeaweb.org/articles?id=10.1257/aer.97.1.354), [Schlenker and Roberts (2009)](https://www.pnas.org/doi/10.1073/pnas.0906865106) and [Burke and Emerick (2016)](https://www.aeaweb.org/articles?id=10.1257/pol.20130025) (henceforth DG, SR and BE respectively) all use data for counties to the east of the 100th parallel, summing up to about 2500 counties. Let's therefore set **N_c = 2500**. They however consider quite substantial different number of years 4, 55 and 24 respectively. Let's choose an intermediate value of **N_t = 25**.\n- **Distribution of the variables**: From the analysis above, we set set `mu_hgdd`, `sigma_hgdd`, `mu_logyield`, `sigma_logyield`, `shape_fe_h`, `scale_fe_h` and `sigma_fe_yield`. Note that I will vary `sigma_fe_yield` and `scale_fe_h` around these values to vary the share of the variance in of $yields$ and $H$ that are location dependent. The variance of the $H$ fixed effects is given by $k\\theta^2$ = `shape_fe_h*scale_fe_h^2`.\n- **Treatment effect**: here, we want to evaluate whether the design of the study would be good enough to detect an effect that is half the size of the obtained estimate. Based on the previous analysis and replication, we can thus set `beta_1` to -0.005/2 = -0.0025. Note that we can also consider other hypothetical true effect sizes (that are consistent with what is found in the literature).\n\nHere is the resulting `baseline_param` dataset:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nbaseline_param <- tibble(\n  N_c = 200,\n  N_t = 4,\n  N_s = 25,\n  mu_h = 68,\n  sigma_h = 30,\n  mu_logyield = 4.55,\n  sigma_logyield = 0.33,\n  beta_1 = -0.003,\n  shape_fe_h = 1.3,\n  scale_fe_h = 20,\n  sigma_fe_yield = 0.10\n)\n\nbaseline_param |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n| N_c| N_t| N_s| mu_h| sigma_h| mu_logyield| sigma_logyield| beta_1| shape_fe_h| scale_fe_h| sigma_fe_yield|\n|---:|---:|---:|----:|-------:|-----------:|--------------:|------:|----------:|----------:|--------------:|\n| 200|   4|  25|   68|      30|        4.55|           0.33| -0.003|        1.3|         20|            0.1|\n\n\n:::\n:::\n\n\nCreate a `generate_data()` function. \n\n## Data exploration\n\nWe can now quickly explore the data we generated. Verify that the distribution are correct, *ie* that the calibration has been done properly. Plot the distributions of the variables and compute their mean and standard deviation.\n\n## Model estimation\n\nWhich model would you estimate? \n\nWrite a function to run the estimation. \n\nThen, write a function that generates a dataset and runs the estimation on this data set. Do you recover the effect of interest?\n\n## All simulations\n\nRepeat the analysis several times, without varying the parameters. Do you retrieve the true effect you put in the data? \n\nIf not, explore why and fix the issue. \n\n## Complexify the DGP\n\nThen, complexify the DGP by adding some correlation\nbut do not change the econometric model. The goal is to explore what happens if the econometric model does not represent the DGP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "sim_2_high_temp_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}