{
  "hash": "3e3aa0c48e4fb6ca43eb3480a9eb9b7b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Low Power and Exaggeration\"\nsubtitle: \"What are some risks of implementing low statistical analyses?\"\npublished-title: \"Date\"\ndate: \"2024-09-25\"\nself-contained: true\nengine: knitr\n---\n\n\n::: callout-tip\n## Objective\n\nAfter this session, you should be able to run a simulation, vary its parameters and have a basic understanding of the statistical power, its drivers and what its lack thereof entails. \n:::\n\n## Summary\n\nIn this session, we focus on **statistical power** and the impact of lack thereof. \n\nFirst, we **study some of its drivers** by expanding our simulation. We **vary parameters values** (sample size, effect size, proportion of treated) and look at their impact on statistical power. Then, we present the related concept of **exaggeration** and start discussing how it may interact with causal identification strategies.\n\n### Session Outline\n\n1.  Summary from last week\n1.  R coding on your own: varying parameters\n1.  A primer on exaggeration\n1.  Introduction to causal exaggeration\n1.  Exercise: implementing a complex simulation\n\n## Materials\n\n`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M64 464c-8.8 0-16-7.2-16-16V64c0-8.8 7.2-16 16-16H224v80c0 17.7 14.3 32 32 32h80V448c0 8.8-7.2 16-16 16H64zM64 0C28.7 0 0 28.7 0 64V448c0 35.3 28.7 64 64 64H320c35.3 0 64-28.7 64-64V154.5c0-17-6.7-33.3-18.7-45.3L274.7 18.7C262.7 6.7 246.5 0 229.5 0H64zm72 208c-13.3 0-24 10.7-24 24V336v56c0 13.3 10.7 24 24 24s24-10.7 24-24V360h44c42 0 76-34 76-76s-34-76-76-76H136zm68 104H160V256h44c15.5 0 28 12.5 28 28s-12.5 28-28 28z\"/></svg>`{=html} [Open slides](../../slides/lecture_3/slides_3_exaggeration.html)\n\n\n::: {.cell}\n<iframe src=\"../../slides/lecture_3/slides_3_exaggeration.html\" width=\"100%\" height=\"400px\" data-external=\"1\"></iframe>\n:::\n\n\n## Exercise\n\nBefore class next week (October 2nd), please send me via email a `html` document, generated with Quarto, implementing the analysis required and answering the questions listed in this [document](ex/ex_1_heterogeneity.qmd).\n\nPlease implement the whole simulation from scratch. You can copy/paste what we have done before and use this as a starting point.\n\n## Specific resources for this lecture\n\n### Publication Bias in Economics\n\n- [Doucouliagos and Stanley (2013)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-6419.2011.00706.x?casa_token=SXb4-OPj5k8AAAAA%3AuMBzYHV8jRno7BnmzQXHkObD53BFX2VBm1g0B7T7psZbaADE14H5dc9xJBNVDR_oF7_tPHEloMen) 60% of research areas in economics feature substantial publication bias (strongest when dominant theory stronger because difficult to defend results that go against it)\n- [Brodeur et al. (2016)](https://www.aeaweb.org/articles?id=10.1257/app.20150044) document publication bias in top econ journals (+ show that it comes more from the author’s side)\n- [Vivalt (2019)](https://onlinelibrary.wiley.com/doi/abs/10.1111/obes.12289) studies the extent of p-hacking in impact evaluations (but decrease over time for RCTs)\n- [Andrews and Kasy (2019)](https://www.aeaweb.org/articles?id=10.1257/aer.20180310): provides a publication bias correction based on the probability of publication conditional on result + method to identify this probability\n- [Brodeur et al. (2020)](https://www.aeaweb.org/articles?id=10.1257/aer.20190687) compare to what extent different causal identification strategies suffer from publication bias. IV (and DiD) suffer more than RCT and RDD\n- [Chopra et al (2023)](https://academic.oup.com/ej/advance-article/doi/10.1093/ej/uead060/7238466) using an experiment with researchers as subject, show that \"studies with a null result are perceived to be less publishable, of lower quality and of lower importance”\n- [Brodeur et al (2023)](https://www.aeaweb.org/articles?id=10.1257/aer.20210795): issues of marginal significance (publication bias) come more from authors’ behavior than from the peer review process\n- Table 2 in [Christensen and Miguel (2018)](http://www.aeaweb.org/articles?id=10.1257/jel.20171350) summarizes this literature\n\n### Evidence of low statistical power (and exaggeration)\n\nIn Economics\n\n- [Ioannidis et al (2017)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjkqPGChsSBAxUwKFkFHeUcAS0QFnoECBIQAQ&url=https%3A%2F%2Facademic.oup.com%2Fej%2Farticle%2F127%2F605%2FF236%2F5069452&usg=AOvVaw38zIIiVe5UOMCjuYq5Qflj&opi=89978449) use meta-analyses to compute the statistical power of the studies “contained” in these meta-analyses\n- [Ferraro and Shukla (2020)](https://www.journals.uchicago.edu/doi/full/10.1093/reep/reaa011) use the same techniques as Ioannidis et al (2017) to show that there are power issues in environmental economics\n- [Ferraro and Shuklla (2022)](https://onlinelibrary.wiley.com/doi/full/10.1002/aepp.13323) same in agricultural economics\n- [DellaVigna and Linos (2022)](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA18709): shows that academic papers studying nudges find effects that are much larger than in large nudge experiment ran by nudges companies. Explain this with low power\n- [Black et al (2022)](https://www.sciencedirect.com/science/article/pii/S0047272722001153): show the importance of taking power into account and show how to implement power calculations\n- [Young (2022)](https://www.sciencedirect.com/science/article/pii/S001429212200054X#sec0010) documents a lack of power of IVs in economics (among other things)\n- In a non-directly related context, [Roth (2023)](https://www.aeaweb.org/articles?id=10.1257/aeri.20210236) underlines that a lack of power of pre-trend tests in event-study designs can lead to bias on the main estimate\n\nIn Political Science\n\n- [Arel-Bundock et al (2022)](https://www.econstor.eu/handle/10419/265531) documents a lack of power in political sciences (median power 10% and only 1 in 10 tests have 80% power to detect the consencessus effects reported in the literature)\n- [Lal et al. (2024)](https://www.cambridge.org/core/journals/political-analysis/article/how-much-should-we-trust-instrumental-variable-estimates-in-political-science-practical-advice-based-on-67-replicated-studies/70A232CA0CD55FE8B97E93543CDD6361#article) documents a lack of power of IVs in political science (among other things)\n- [Stommes et al (2023)](https://journals.sagepub.com/doi/full/10.1177/20531680231166457) shows in RD in political sciences are under-powered to detect anything but large effects and lead to exaggeration\n\n### Mechanisms behind exaggeration\n\n- [Gelman and Tuerlinckx (2000)](https://doi.org/10.1007/s001800000040) and [Gelman and Carlin (2014)](https://doi.org/10.1177/1745691614551642) introduce the concept of Type-M error (exaggeration)\n- [Lu et al (2019)](http://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12132) and [Zwet and Cator (2021)](https://onlinelibrary.wiley.com/doi/10.1111/stan.12241) derive mathematical proof of the evolution of exaggeration with effect size and precision of the estimator\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}